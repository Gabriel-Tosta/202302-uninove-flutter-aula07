{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU/PGT9t0lFVt4O9W30qII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabriel-Tosta/202302-uninove-flutter-aula07/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLVp1F63rhpB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt sasasasas\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Versão do TensorFlow: {tf.__version__}\")\n",
        "print(f\"Versão do Keras: {keras.__version__}\")\n",
        "\n",
        "# 1. Carregar o Conjunto de Dados MNIST\n",
        "# O MNIST é um dataset de dígitos manuscritos (0-9)\n",
        "# Ele já vem dividido em conjuntos de treino e teste\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Exibir as dimensões dos dados\n",
        "print(f\"Shape de x_train: {x_train.shape}\") # (60000, 28, 28) -> 60k imagens de 28x28 pixels\n",
        "print(f\"Shape de y_train: {y_train.shape}\") # (60000,) -> 60k labels\n",
        "print(f\"Shape de x_test: {x_test.shape}\")   # (10000, 28, 28)\n",
        "print(f\"Shape de y_test: {y_test.shape}\")   # (10000,)\n",
        "\n",
        "# Opcional: Visualizar algumas imagens do dataset\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(x_train[i], cmap='gray') # Exibe a imagem em escala de cinza\n",
        "    plt.title(f\"Label: {y_train[i]}\")   # Exibe o rótulo da imagem\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Exemplos de Dígitos MNIST\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Pré-processar os Dados\n",
        "# As redes neurais geralmente esperam dados normalizados e \"achatados\" (flattened)\n",
        "\n",
        "# a) Normalização: Reduzir os valores dos pixels de 0-255 para 0-1\n",
        "# Isso ajuda o modelo a aprender de forma mais estável\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# b) Achatamento (Flatten): Converter a imagem 2D (28x28) em um vetor 1D (784)\n",
        "# Uma MLP espera um vetor de entrada\n",
        "x_train = x_train.reshape(-1, 28 * 28) # -1 significa que Keras calcula o tamanho da primeira dimensão\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "print(f\"\\nShape de x_train após flatten: {x_train.shape}\")\n",
        "print(f\"Shape de x_test após flatten: {x_test.shape}\")\n",
        "\n",
        "# c) Codificação One-Hot para os Rótulos (Labels)\n",
        "# O Keras espera que as classes de saída estejam em formato one-hot encoding para classificação multiclasse\n",
        "# Ex: O dígito 5 seria [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "num_classes = 10 # Dígitos de 0 a 9\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"Shape de y_train após one-hot encoding: {y_train.shape}\")\n",
        "print(f\"Shape de y_test após one-hot encoding: {y_test.shape}\")\n",
        "\n",
        "# 3. Construir o Modelo da Rede Neural com Keras\n",
        "# Usaremos o modelo Sequential, que é uma pilha linear de camadas\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        # Camada de entrada (Flatten): Acha a imagem de 28x28 para um vetor de 784 pixels\n",
        "        # input_shape=(28 * 28,) define o formato esperado da entrada\n",
        "        layers.Input(shape=(28 * 28,)), # Alternativa a layers.Flatten(input_shape=(28, 28)) para MLP\n",
        "\n",
        "        # Camada Oculta 1 (Dense): Uma camada densamente conectada com 256 neurônios\n",
        "        # activation='relu' é uma função de ativação comum para camadas ocultas\n",
        "        layers.Dense(256, activation=\"relu\", name=\"hidden_layer_1\"),\n",
        "\n",
        "        # Camada Oculta 2 (Dense): Outra camada densamente conectada com 128 neurônios\n",
        "        layers.Dense(128, activation=\"relu\", name=\"hidden_layer_2\"),\n",
        "\n",
        "        # Camada de Saída (Dense): 10 neurônios, um para cada dígito (0-9)\n",
        "        # activation='softmax' é usada para problemas de classificação multiclasse\n",
        "        # Ela produz uma distribuição de probabilidades para as 10 classes\n",
        "        layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Exibir o sumário do modelo para ver a arquitetura e o número de parâmetros\n",
        "model.summary()\n",
        "\n",
        "# 4. Compilar o Modelo\n",
        "# A fase de compilação configura o processo de aprendizado do modelo\n",
        "\n",
        "model.compile(\n",
        "    # Optimizer: Algoritmo usado para ajustar os pesos da rede (Adam é uma boa escolha geral)\n",
        "    optimizer=\"adam\",\n",
        "    # Loss function: Mede o quão bom (ou ruim) o modelo está performando\n",
        "    # categorical_crossentropy é para classificação multiclasse com labels one-hot\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    # Metrics: Medidas para avaliar o desempenho do modelo durante o treino e teste (precisão)\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# 5. Treinar o Modelo\n",
        "# O treinamento é o processo onde o modelo aprende com os dados de treino\n",
        "\n",
        "batch_size = 128  # Número de amostras por atualização de gradiente\n",
        "epochs = 10       # Número de vezes que o modelo verá todo o conjunto de dados de treino\n",
        "\n",
        "print(f\"\\nIniciando o treinamento do modelo com {epochs} épocas...\")\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "# 6. Avaliar o Modelo\n",
        "# Avalia o desempenho do modelo nos dados de teste (que ele nunca viu antes)\n",
        "\n",
        "print(\"\\nAvaliando o modelo nos dados de teste...\")\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Perda no teste: {score[0]:.4f}\")\n",
        "print(f\"Precisão no teste: {score[1]:.4f}\")\n",
        "\n",
        "# 7. Fazer Previsões (Opcional)\n",
        "# Exemplo de como usar o modelo treinado para fazer previsões em novas imagens\n",
        "\n",
        "# Pegar algumas imagens do conjunto de teste\n",
        "num_predictions = 5\n",
        "sample_images = x_test[:num_predictions]\n",
        "sample_labels = np.argmax(y_test[:num_predictions], axis=1) # Converte one-hot de volta para label numérica\n",
        "\n",
        "# O modelo prevê probabilidades para cada classe\n",
        "predictions = model.predict(sample_images)\n",
        "\n",
        "print(f\"\\nFazendo previsões para {num_predictions} imagens de teste:\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(num_predictions):\n",
        "    predicted_label = np.argmax(predictions[i]) # Pega a classe com a maior probabilidade\n",
        "    true_label = sample_labels[i]\n",
        "\n",
        "    plt.subplot(1, num_predictions, i + 1)\n",
        "    plt.imshow(sample_images[i].reshape(28, 28), cmap='gray') # Redimensiona de volta para 28x28 para exibir\n",
        "    plt.title(f\"Real: {true_label}\\nPredito: {predicted_label}\",\n",
        "              color='green' if predicted_label == true_label else 'red')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Previsões do Modelo\", y=0.95)\n",
        "plt.show()\n",
        "\n",
        "# 8. Visualizar o Histórico de Treinamento (Opcional)\n",
        "# Plotar a perda e a precisão ao longo das épocas\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot da Perda\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Perda de Treino')\n",
        "plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
        "plt.title('Perda durante o Treinamento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Perda')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot da Precisão\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Precisão de Treino')\n",
        "plt.plot(history.history['val_accuracy'], label='Precisão de Validação')\n",
        "plt.title('Precisão durante o Treinamento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Precisão')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}